{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 3 - Redes Neurales Profundas\n",
    "Raul Jimenez 19017\n",
    "\n",
    "Donaldo Garcia 19683"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo que se desarrolló en clase tiene una precisión ya bastante alta.  Sin\n",
    "embargo, hay varios ajustes que se pueden intentar para mejorarlo.\n",
    "Es importante poner atención al tiempo que se tarda cada época en ejecutar.\n",
    "Utilizando el código visto en clase, experimenten con  los hiperparámetros del \n",
    "algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. El ancho (tamaño de la capa escondida) del algoritmo. Intenten con un tamaño de 200.  ¿Cómo cambia la precisión de validación del modelo?  ¿Cuánto tiempo se tardó el algoritmo en entrenar?  ¿Puede encontrar un tamaño de capa escondida que funcione mejor?\n",
    "R/La precision aumenta de un 96.6% a un 97.92%. Con 200 como tamaño de capas se tardo 12.8s, este disminuyo ya que con 50 capas escondidas se tardaba 13.6. El numero de capa escondida que se encontro fue de 350, con este tamaño llega una precision de 98.01%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. La profundidad del algoritmo.  Agreguen una capa escondida más al algoritmo. Este es un ejercicio extremadamente importante!  ¿Cómo cambia la precisión de validación?  ¿Qué hay del tiempo que se tarda en ejecutar?   Pista:  deben tener cuidado con las formas de los pesos y los sesgos.\n",
    "R/ Agregando una capa extra se pudo notar que hubo un aumento moderado en el porcentaje de precision. Cabe destacar de igual manera que el tiempo que se tarda en ejecutar esto es menor, el tiempo con esta nueva capa es de 9.9s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. El ancho y la profundidad del algoritmo.  Agregue cuantas capas sean necesarias para llegar a 5 capas escondidas.  Es más, ajusten el ancho del algoritmo conforme lo encuentre más conveniente.  ¿Cómo cambia la precisión de validación? ¿Qué hay del tiempo de ejecución?\n",
    "R/ Cuando le agregamos mas capas escondidas tuvimos que aumentar de igual forma el ancho para poder obtener mejores resulados. Con 5 capas escondidas el mejor ancho que se encontro fue de 440 para llegar a una precision de 98.04%. Sin embargo el timpo de ejecicion al tener mas capas escondidas y mas ancho aumento a 24.6s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Experimenten con las funciones de activación.  Intenten aplicar una transformación sigmoidal a ambas capas.  La activación sigmoidal se obtiene escribiendo “sigmoid”.\n",
    "R/ Aplicando la transformacion de sigmoidal se puede observar que el rendimiento disminuye. Ahora se llega a una precision de 95.23%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Continúen experimentando con las funciones de activación.  Intenten aplicar un ReLu a la primera capa escondida y tanh a la segunda.  La activación tanh se obtiene escribiendo “tanh”.\n",
    "R/ Se puede observar que al aplicar en la segunda capa tanh no muestra mayor diferencia a como estaba originalmente con ambas capas con \"relu\". El porcentaje con este cambio es de 97.19%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ajusten el tamaño de la tanda.  Prueben con un tamaño de tanda de 10,000. ¿Cómo cambia el tiempo requerido?  ¿Cómo cambia la precisión?\n",
    "R/ Pérdida de prueba: 0.23. Precisión de prueba: 93.08%\n",
    "Con 10 mil tandas hubo una disminución de tiempo en el entrenamiento, de 6.8 segundos a 4.5. Sin embargo, hubo una disminición de 3% aproximadamente en precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ajusten el tamaño de la tanda a 1.  Eso corresponde al SGD. ¿Cómo cambian el tiempo y la precisión?  ¿Es el resultado coherente con la teoría?\n",
    "R/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Ajusten la tasa de aprendizaje.  Prueben con un valor de 0.0001.  ¿Hace alguna diferencia?\n",
    "R/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Ajusten la tasa de aprendizaje a 0.02.  ¿Hay alguna diferencia?\n",
    "R/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Combinen todos los métodos indicados arriba e intenten llegar a una precisión de validación de 98.5% o más.\n",
    "R/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODIGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to ~\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62a2912579a4e87a03e2944eb4323ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175af6d7318b4fc6a8c57389b6bc4f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2b675eb2cc4463ab1cf04585882b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d09b317af447beb1911f8685af09cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f5c19d7a2c4dddbf904ed253c22741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a5f87eede249a59384f425dfc1a2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~\\tensorflow_datasets\\mnist\\3.0.1.incompleteEE5XZ8\\mnist-train.tfrecord*...:   0%|          | 0/6000…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotFoundError",
     "evalue": "Failed to create a NewWriteableFile: ~\\tensorflow_datasets\\mnist\\3.0.1.incompleteEE5XZ8\\mnist-train.tfrecord-00000-of-00001 : The system cannot find the path specified.\r\n; No such process",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ALIEWARE\\Documents\\Raul_Angel\\Universidad_del_Valle_de_Guatemala_Ciencias_de_la_computacion_y_tecnologia\\8vo_Semestre\\Data Science\\Laboratorio-3---Redes-Neurales-Profundas\\lab3.ipynb Cell 15\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ALIEWARE/Documents/Raul_Angel/Universidad_del_Valle_de_Guatemala_Ciencias_de_la_computacion_y_tecnologia/8vo_Semestre/Data%20Science/Laboratorio-3---Redes-Neurales-Profundas/lab3.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m datos_mnist, info_mnist \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39;49mload(name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmnist\u001b[39;49m\u001b[39m'\u001b[39;49m, shuffle_files \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m, as_supervised\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\ALIEWARE\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:327\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m download:\n\u001b[0;32m    326\u001b[0m   download_and_prepare_kwargs \u001b[39m=\u001b[39m download_and_prepare_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m--> 327\u001b[0m   dbuilder\u001b[39m.\u001b[39mdownload_and_prepare(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs)\n\u001b[0;32m    329\u001b[0m \u001b[39mif\u001b[39;00m as_dataset_kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m   as_dataset_kwargs \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\ALIEWARE\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:481\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[0;32m    479\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mread_from_directory(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_dir)\n\u001b[0;32m    480\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 481\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[0;32m    482\u001b[0m       dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[0;32m    483\u001b[0m       download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m    484\u001b[0m   )\n\u001b[0;32m    486\u001b[0m   \u001b[39m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[0;32m    487\u001b[0m   \u001b[39m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[0;32m    488\u001b[0m   \u001b[39m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[0;32m    489\u001b[0m   \u001b[39m# when reading from package data.\u001b[39;00m\n\u001b[0;32m    490\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdownload_size \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mdownloaded_size\n",
      "File \u001b[1;32mc:\\Users\\ALIEWARE\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1218\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1207\u001b[0m   \u001b[39mfor\u001b[39;00m split_name, generator \u001b[39min\u001b[39;00m utils\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   1208\u001b[0m       split_generators\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1209\u001b[0m       desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerating splits...\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1210\u001b[0m       unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m splits\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1211\u001b[0m       leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1212\u001b[0m   ):\n\u001b[0;32m   1213\u001b[0m     filename_template \u001b[39m=\u001b[39m naming\u001b[39m.\u001b[39mShardedFileTemplate(\n\u001b[0;32m   1214\u001b[0m         split\u001b[39m=\u001b[39msplit_name,\n\u001b[0;32m   1215\u001b[0m         dataset_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[0;32m   1216\u001b[0m         data_dir\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_path,\n\u001b[0;32m   1217\u001b[0m         filetype_suffix\u001b[39m=\u001b[39mpath_suffix)\n\u001b[1;32m-> 1218\u001b[0m     future \u001b[39m=\u001b[39m split_builder\u001b[39m.\u001b[39;49msubmit_split_generation(\n\u001b[0;32m   1219\u001b[0m         split_name\u001b[39m=\u001b[39;49msplit_name,\n\u001b[0;32m   1220\u001b[0m         generator\u001b[39m=\u001b[39;49mgenerator,\n\u001b[0;32m   1221\u001b[0m         filename_template\u001b[39m=\u001b[39;49mfilename_template,\n\u001b[0;32m   1222\u001b[0m         disable_shuffling\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49mdisable_shuffling,\n\u001b[0;32m   1223\u001b[0m     )\n\u001b[0;32m   1224\u001b[0m     split_info_futures\u001b[39m.\u001b[39mappend(future)\n\u001b[0;32m   1226\u001b[0m \u001b[39m# Process the result of the beam pipeline.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ALIEWARE\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:310\u001b[0m, in \u001b[0;36mSplitBuilder.submit_split_generation\u001b[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[39m# Depending on the type of generator, we use the corresponding\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39m# `_build_from_xyz` method.\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(generator, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mIterable):\n\u001b[1;32m--> 310\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_from_generator(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbuild_kwargs)\n\u001b[0;32m    311\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Otherwise, beam required\u001b[39;00m\n\u001b[0;32m    312\u001b[0m   unknown_generator_type \u001b[39m=\u001b[39m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInvalid split generator value for split `\u001b[39m\u001b[39m{\u001b[39;00msplit_name\u001b[39m}\u001b[39;00m\u001b[39m`. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    314\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mExpected generator or apache_beam object. Got: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    315\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(generator)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ALIEWARE\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:383\u001b[0m, in \u001b[0;36mSplitBuilder._build_from_generator\u001b[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[0;32m    381\u001b[0m     utils\u001b[39m.\u001b[39mreraise(e, prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFailed to encode example:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mexample\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    382\u001b[0m   writer\u001b[39m.\u001b[39mwrite(key, example)\n\u001b[1;32m--> 383\u001b[0m shard_lengths, total_size \u001b[39m=\u001b[39m writer\u001b[39m.\u001b[39;49mfinalize()\n\u001b[0;32m    385\u001b[0m split_info \u001b[39m=\u001b[39m splits_lib\u001b[39m.\u001b[39mSplitInfo(\n\u001b[0;32m    386\u001b[0m     name\u001b[39m=\u001b[39msplit_name,\n\u001b[0;32m    387\u001b[0m     shard_lengths\u001b[39m=\u001b[39mshard_lengths,\n\u001b[0;32m    388\u001b[0m     num_bytes\u001b[39m=\u001b[39mtotal_size,\n\u001b[0;32m    389\u001b[0m     filename_template\u001b[39m=\u001b[39mfilename_template,\n\u001b[0;32m    390\u001b[0m )\n\u001b[0;32m    391\u001b[0m \u001b[39mreturn\u001b[39;00m _SplitInfoFuture(\u001b[39mlambda\u001b[39;00m: split_info)\n",
      "File \u001b[1;32mc:\\Users\\ALIEWARE\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow_datasets\\core\\writer.py:303\u001b[0m, in \u001b[0;36mWriter.finalize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[39mfor\u001b[39;00m shard_spec \u001b[39min\u001b[39;00m shard_specs:\n\u001b[0;32m    301\u001b[0m   iterator \u001b[39m=\u001b[39m itertools\u001b[39m.\u001b[39mislice(examples_generator, \u001b[39m0\u001b[39m,\n\u001b[0;32m    302\u001b[0m                               shard_spec\u001b[39m.\u001b[39mexamples_number)\n\u001b[1;32m--> 303\u001b[0m   record_keys \u001b[39m=\u001b[39m _write_examples(shard_spec\u001b[39m.\u001b[39;49mpath, iterator,\n\u001b[0;32m    304\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_file_format)\n\u001b[0;32m    306\u001b[0m   \u001b[39m# No shard keys returned (e.g: TFRecord format), index cannot be\u001b[39;00m\n\u001b[0;32m    307\u001b[0m   \u001b[39m# created.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m record_keys:\n",
      "File \u001b[1;32mc:\\Users\\ALIEWARE\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow_datasets\\core\\writer.py:168\u001b[0m, in \u001b[0;36m_write_examples\u001b[1;34m(path, iterator, file_format)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_write_examples\u001b[39m(\n\u001b[0;32m    163\u001b[0m     path: epath\u001b[39m.\u001b[39mPathLike,\n\u001b[0;32m    164\u001b[0m     iterator: Iterable[type_utils\u001b[39m.\u001b[39mKeySerializedExample],\n\u001b[0;32m    165\u001b[0m     file_format: file_adapters\u001b[39m.\u001b[39mFileFormat \u001b[39m=\u001b[39m file_adapters\u001b[39m.\u001b[39mDEFAULT_FILE_FORMAT\n\u001b[0;32m    166\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[file_adapters\u001b[39m.\u001b[39mExamplePositions]:\n\u001b[0;32m    167\u001b[0m   \u001b[39m\"\"\"Write examples from iterator in the given `file_format`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m   \u001b[39mreturn\u001b[39;00m file_adapters\u001b[39m.\u001b[39;49mADAPTER_FOR_FORMAT[file_format]\u001b[39m.\u001b[39;49mwrite_examples(\n\u001b[0;32m    169\u001b[0m       path, iterator)\n",
      "File \u001b[1;32mc:\\Users\\ALIEWARE\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow_datasets\\core\\file_adapters.py:119\u001b[0m, in \u001b[0;36mTfRecordFileAdapter.write_examples\u001b[1;34m(cls, path, iterator)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite_examples\u001b[39m(\n\u001b[0;32m    106\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m    107\u001b[0m     path: epath\u001b[39m.\u001b[39mPathLike,\n\u001b[0;32m    108\u001b[0m     iterator: Iterable[type_utils\u001b[39m.\u001b[39mKeySerializedExample],\n\u001b[0;32m    109\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[ExamplePositions]:\n\u001b[0;32m    110\u001b[0m   \u001b[39m\"\"\"Write examples from given iterator in given path.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \n\u001b[0;32m    112\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39m    None\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m   \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mTFRecordWriter(os\u001b[39m.\u001b[39;49mfspath(path)) \u001b[39mas\u001b[39;00m writer:\n\u001b[0;32m    120\u001b[0m     \u001b[39mfor\u001b[39;00m _, serialized_example \u001b[39min\u001b[39;00m iterator:\n\u001b[0;32m    121\u001b[0m       writer\u001b[39m.\u001b[39mwrite(serialized_example)\n",
      "File \u001b[1;32mc:\\Users\\ALIEWARE\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\lib\\io\\tf_record.py:294\u001b[0m, in \u001b[0;36mTFRecordWriter.__init__\u001b[1;34m(self, path, options)\u001b[0m\n\u001b[0;32m    291\u001b[0m   options \u001b[39m=\u001b[39m TFRecordOptions(compression_type\u001b[39m=\u001b[39moptions)\n\u001b[0;32m    293\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m \u001b[39msuper\u001b[39;49m(TFRecordWriter, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    295\u001b[0m     compat\u001b[39m.\u001b[39;49mas_bytes(path), options\u001b[39m.\u001b[39;49m_as_record_writer_options())\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a NewWriteableFile: ~\\tensorflow_datasets\\mnist\\3.0.1.incompleteEE5XZ8\\mnist-train.tfrecord-00000-of-00001 : The system cannot find the path specified.\r\n; No such process"
     ]
    }
   ],
   "source": [
    "datos_mnist, info_mnist = tfds.load(name='mnist', shuffle_files = False, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entreno_mnist, prueba_mnist = datos_mnist['train'], datos_mnist['test']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f25a96a986ade36c04481765247125034b39d1e9e6e631e58762e8ff940967bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
